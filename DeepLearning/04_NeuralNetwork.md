# Perceptron & Neural Network

> ê°•ì˜ ìë£Œ : ë”¥ëŸ¬ë‹ (2020-1) 

## Perceptron

ë‹¤ìˆ˜ì˜ ì‹ í˜¸ (Input) ì„ ì…ë ¥ë°›ì•„ì„œ í•˜ë‚˜ì˜ ì‹ í˜¸ (Output) ì„ ì¶œë ¥í•œë‹¤.    
perceptron ì˜ ì¶œë ¥ê°’ì€ 1 ë˜ëŠ” 0 ì´ê¸° ë•Œë¬¸ì— ì„ í˜• ë¶„ë¥˜ (linear classifier) ëª¨í˜•ì´ë‹¤.

<img src="./screenshot/04_nn4.png" width="700">

<br/>

ì²˜ìŒì—ëŠ” ì„ì˜ë¡œ ì„¤ì •ëœ weightë¡œ ì‹œì‘í•œë‹¤.   
í•™ìŠµ ë°ì´í„°ë¥¼ perceptron ëª¨í˜•ì— ì…ë ¥í•˜ë©° ë¶„ë¥˜ê°€ ì˜ ëª» ë˜ì—ˆì„ ë•Œ weightì„ ê°œì„ í•´ ë‚˜ê°„ë‹¤. 

- **ê°€ì¤‘ì¹˜** (weight) : ì…ë ¥ ì‹ í˜¸ê°€ ê²°ê³¼ ì¶œë ¥ì— ì£¼ëŠ” ì˜í–¥ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” ë§¤ê°œë³€ìˆ˜
- **ë³€í–¥** (bias) : ë‰´ëŸ°ì´ ì–¼ë§ˆë‚˜ ì‰½ê²Œ í™œì„±í™” ë˜ëŠëƒë¥¼ ì¡°ì ˆí•˜ëŠ” ë§¤ê°œë³€ìˆ˜

í¼ì…‰íŠ¸ë¡ ì€ ì„ í˜•ë¶„ë¥˜ëŠ” í•  ìˆ˜ ìˆì§€ë§Œ ë¹„ì„ í˜•ë¶„ë¥˜ëŠ” ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” í•œê³„ì ì´ ìˆë‹¤.   
ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì„ ë§Œë“ ë‹¤. ì¸µì„ ê²¹ê²¹ì´ ìŒ“ì•„ë‚˜ê°€ëŠ” ê²ƒì´ë‹¤.   

ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì€ `step function` (ì„ê³„ê°’ì„ ë„˜ì–´ì„°ì„ ë•Œ ì¶œë ¥ì„ 1ë¡œ í•¨) ì„ í™œì„±í™” í•¨ìˆ˜ë¡œ ì‚¬ìš©í•œë‹¤.   
ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì€ ì¸µì´ ì—¬ëŸ¬ê°œì´ë©° `sigmoid function` ì„ í™œì„±í™” í•¨ìˆ˜ë¡œ ì‚¬ìš©í•œë‹¤.

<br/>

## Neural Network

Perceptronì´ ê°€ì¤‘ì¹˜ë¥¼ ì§ì ‘ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ì‘ì—…ì„ í•œë‹¤ëŠ” í•œê³„ì ì„ í•´ê²°í•œ ë°©ë²•ì´ **ì‹ ê²½ë§** ì´ë‹¤.   
ì•Œì•„ì„œ ê°€ì¤‘ì¹˜ ê°’ì„ ì„¤ì •í•˜ê³  ì¡°ì •í•˜ëŠ” ê²ƒì´ (ìë™ìœ¼ë¡œ í•™ìŠµ) ì‹ ê²½ë§ì˜ í° íŠ¹ì§•ì´ë‹¤

<img src="./screenshot/04_nn6.png" height="200"> <img src="./screenshot/04_nn7.png" height="200">

ì‹ ê²½ë§ì€ Input(ì…ë ¥ì¸µ), Hidden(ì€ë‹‰ì¸µ), Output(ì¶œë ¥ì¸µ) ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

<br/>

### ReLU Function

<img src="./screenshot/04_nn9.png" width="300">

ì…ë ¥ì´ 0ì„ ë„˜ìœ¼ë©´ ê·¸ ì…ë ¥ì„ ê·¸ëŒ€ ì¶œë ¥í•˜ê³ , 0 ì´í•˜ë©´ 0ì„ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.


### Sigmoid Function 

<img src="./screenshot/04_nn5.png" width="200">

<img src="./screenshot/04_nn8.png" width="400">

ê°€ì¤‘ì¹˜ ê°’ì„ ì „ë‹¬í•  ë•Œ ì¢€ ë” ë¶€ë“œëŸ½ê²Œ ì–‘ì„ ì¡°ì ˆí•´ ì „ë‹¬í•œë‹¤.

```python
import numpy as np
import matplotlib.pylab as plt

def sigmoid(x):
    return 1 / (1+np.exp(-x))

print(sigmoid(1))
print(sigmoid(np.array([0,0.5,1])))

x = np.arange(-10.0, 10.0, 0.1)
y = sigmoid(x)

plt.plot(x,y)
plt.ylim(-0.1, 1.1)
plt.show()
```
```profile
0.7310585786300049
[0.5        0.62245933 0.73105858]
```

<img src="./screenshot/04_nn.png" width="500">

<br/>

### ğŸ“Œ ë‹¨ìˆœ ì‹ ê²½ë§ êµ¬ì¡°

<img src="./screenshot/04_nn2.png" width="400">

```python
import numpy as np

# ë‹¨ìˆœ ì‹ ê²½ë§ êµ¬ì¡°
def init_network():
    network = {}
    network['W'] = np.array([
        [0.2, 0.5, 0.3],
        [0.8, 0.6, 0.4]
    ])
    return network

network = init_network()
```

<br/>

### ğŸ“Œ Forward Pass

```python
# forward pass 1
def forward1(network, x):
    W = network['W']
    y = np.array([0.0, 0.0, 0.0])
    y[0] = W[0][0] * x[0] + W[1][0] * x[1]
    y[1] = W[0][1] * x[0] + W[1][1] * x[1]
    y[2] = W[0][2] * x[0] + W[1][2] * x[1]
    return y
    
# forward pass 2
def forward2(network,x):
    y = np.dot(x, network['W'])
    return y
    
network = init_network()
y = forward(network, np.array([1.0, 2.0]))
print(y)
```
```profile
[1.8, 1.7, 1.1]
```
<br/>

### ğŸ“Œ Active Function ì ìš©

```python
import numpy as np

# ë‹¨ìˆœ ì‹ ê²½ë§ êµ¬ì¡°
def init_network():
    network = {}
    network['W'] = np.array([
        [0.2, 0.5, 0.3],
        [0.8, 0.6, 0.4]
    ])
    return network

network = init_network()

# forward pass 1
def forward1(network, x):
    W = network['W']
    y = np.array([0.0, 0.0, 0.0])
    y[0] = W[0][0] * x[0] + W[1][0] * x[1]
    y[1] = W[0][1] * x[0] + W[1][1] * x[1]
    y[2] = W[0][2] * x[0] + W[1][2] * x[1]
    return y


# forward pass 2
def forward2(network,x):
    y = np.dot(x, network['W'])
    return y

# sigmoid : ì¶œë ¥ê°’ì„ 0~1ë¡œ ì¡°ì •
def sigmoid(x):
    return 1 / ( 1 + np.exp(-x) )

# forward + sigmoid
def forward3(network,x):
    y = sigmoid(np.dot(x, network['W']))
    return y

# softmax : ë§ˆì§€ë§‰ ì¶œë ¥ì¸µ. ì¶œë ¥ì˜ í•©ê³„ëŠ” 1
# overflow ì¡°ì •í•´ì¤Œ
# ë§ˆì§€ë§‰ ì¶œë ¥ì¸µì—ì„œ í™œì„±í™” í•¨ìˆ˜ë¡œ ì‚¬ìš© 
def softmax(x)  :
    e = np.exp(x - np.max(x))
    s = np.sum(e)
    return e / s


y = forward2(network, np.array([1.0, 2.0]))
print(y)
print()

y2 = forward3(network, np.array([1.0, 2.0]))
print(y2)
print()
```
```profile
[1.8 1.7 1.1]

[0.85814894 0.84553473 0.75026011]
```

<br/>

### ğŸ“Œ SoftMax Function

- Neural Network ê¸°ë°˜ multi-label classifier ì˜ ë§ˆì§€ë§‰ ì¸µì— ì‚¬ìš©
- Cross - entropy loss ì™€ í•¨ê»˜ ì“°ì„
- ì¶œë ¥ ê°’ì´ 0ë¶€í„° 1 ì‚¬ì´ì˜ ê°’
- ì¶œë ¥ì˜ í•©ê³„ëŠ” 1

```python
# overflow ì¡°ì •í•´ì¤Œ
# ë§ˆì§€ë§‰ ì¶œë ¥ì¸µì—ì„œ í™œì„±í™” í•¨ìˆ˜ë¡œ ì‚¬ìš© 
def softmax(x)  :
    e = np.exp(x - np.max(x))
    s = np.sum(e)
    return e / s
```
