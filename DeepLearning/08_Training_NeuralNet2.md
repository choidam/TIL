# Training NeuralNet (2) 

## Weight Initialization

ë”¥ëŸ¬ë‹ í•™ìŠµì— ìˆì–´ì„œ ì´ˆê¸° ê°€ì¤‘ì¹˜ ì„¤ì •ì€ ë§¤ìš° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤.   
ì´ˆê¸°ê°’ ì„¤ì •ì„ ì˜ëª»í•´ ë¬¸ì œê°€ ë°œìƒí•˜ëŠ” ê²½ìš°ë“¤ì„ ì‚´í´ë³´ì.

### ë¬¸ì œ ë°œìƒ 1. W ê°€ ëª¨ë‘ ê°™ì€ ê²½ìš° (í˜¹ì€ ë‹¤ 0ì¸ ê²½ìš°)

<img src="./screenshot/08_nn01.png" width="300">

ë§Œì•½ ë°ì´í„°ë¥¼ í‰ê·  0ìœ¼ë¡œ ì •ê·œí™” ì‹œí‚¨ë‹¤ë©´, ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™” ì‹œí‚¨ë‹¤ëŠ” ê²ƒì´ í•©ë¦¬ì ìœ¼ë¡œ ë³´ì¼ ìˆ˜ ìˆë‹¤.   
ê·¸ëŸ¬ë‚˜ ì‹¤ì œë¡œ 0ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í•˜ë©´ ëª¨ë“  ë‰´ëŸ°ë“¤ì´ ê°™ì€ ê°’ì„ ë‚˜íƒ€ë‚¼ ê²ƒì´ê³ , ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ê° ê°€ì¤‘ì¹˜ì˜ update ê°€ ë™ì¼í•˜ê²Œ ì´ë¤„ì§ˆ ê²ƒì´ë‹¤. ê²°êµ­ ì œëŒ€ë¡œ í•™ìŠµí•˜ê¸° ì–´ë ¤ìš¸ ê²ƒì´ë‹¤.

### ë¬¸ì œ ë°œìƒ 2. small random number ì„ ì¤¬ì„ ê²½ìš°

```python
import numpy as np
from matplotlib.pylab import plt

# assume some unit gaussian 10-D input data

D = np.random.randn(1000, 500)
hidden_layer_sizes = [500]*10
nonlinearities = ['tanh']*len(hidden_layer_sizes)

act = {'relu': lambda x:np.maximum(0,x), 'tanh':lambda x: np.tanh(x)}
Hs = {}

for i in range(len(hidden_layer_sizes)):
    X = D if i==0 else Hs[i-1] # input at this layer
    fan_in = X.shape[1]
    fan_out = hidden_layer_sizes[i]
    W = np.random.randn(fan_in, fan_out) * 0.01 # layer initialization

    H = np.dot(X, W) # matrix multiply
    H = act[nonlinearities[i]](H) # nonlinearity
    Hs[i] = H # cache result on this layer

print('input layer had mean', np.mean(D), 'and std', np.std(D))

# look at distributions at each layer
layer_means = [np.mean(H) for i,H in Hs.items()]
layer_stds = [np.std(H) for i,H in Hs.items()]

# print
for i,H in Hs.items() :
    print('hidden layer', i+1, 'had mean', layer_means[i], 'and std', layer_stds[i])

plt.figure()
plt.subplot(1,2,1)
plt.title("layer mean")
plt.plot(range(10), layer_means, 'ob-')
plt.subplot(1,2,2)
plt.title("layer std")
plt.plot(range(10), layer_stds, 'or-')

plt.show()

plt.figure()
for i,H in Hs.items() :
    plt.subplot(1,len(Hs), i+1)
    plt.hist(H.ravel(), 30, range=(-1,1))
plt.show()
```

> ì˜ˆì‹œ ì½”ë“œëŠ” python 2  ê¸°ì¤€ì´ë¼ì„œ ë¬¸ë²• ëª‡ ë¶€ë¶„ì„ ìˆ˜ì •í–ˆë‹¤..

```profile
input layer had mean 0.00031233516316648657 and std 1.0006710331935624
hidden layer 1 had mean 0.0032899684992063636 and std 0.9820452751182597
hidden layer 2 had mean 0.003931758696050947 and std 0.9819251097074527
hidden layer 3 had mean 0.0026063736923909596 and std 0.9817340581953854
hidden layer 4 had mean 0.001848042971495998 and std 0.9817977909967206
hidden layer 5 had mean -0.0016554517737738606 and std 0.9815496358911521
hidden layer 6 had mean 0.0006604474921595372 and std 0.9816919440654884
hidden layer 7 had mean 0.0001594571291677151 and std 0.9816941165792189
hidden layer 8 had mean -0.0011475135707183642 and std 0.9816304323127083
hidden layer 9 had mean 0.0018214029856872354 and std 0.9815362486244705
hidden layer 10 had mean -0.0013684972820372828 and std 0.981809472200405
```

<img src="./screenshot/08_nn03.png" width="500">

<img src="./screenshot/08_nn02.png" width="700">

ì²˜ìŒ layerì—ëŠ” ì •ê·œ ë¶„í¬ê°€ ì˜ ì´ë£¨ì–´ì§€ì§€ë§Œ layerê°€ ìŒ“ì¼ ìˆ˜ë¡ 0ì— ë­‰ì¹œë‹¤.

xê°€ 0ì— ê°€ê¹Œì›Œì§€ë©´ gradientë„ 0ì— ê°€ê¹Œì›Œì ¸ì„œ Vanishing gradient ê°€ ë°œìƒí•œë‹¤.

### ë¬¸ì œ ë°œìƒ 3. 2ì˜ case ì—ì„œ 0.01 ëŒ€ì‹  1ì„ ê³±í–ˆì„ ê²½ìš°

```python
    W = np.random.randn(fan_in, fan_out) * 1.0 # layer initialization
```

```profile
input layer had mean -0.0004322766817344249 and std 1.0010016502837895
hidden layer 1 had mean -0.0012174416683021685 and std 0.981880039850602
hidden layer 2 had mean -0.0015410543140147272 and std 0.9814327560617824
hidden layer 3 had mean -0.00031650342142403704 and std 0.9817555532084202
hidden layer 4 had mean -0.002599692767592639 and std 0.9817765320254677
hidden layer 5 had mean 0.00021900665038415243 and std 0.9816731709326508
hidden layer 6 had mean 0.0027174686541096625 and std 0.9817325832571894
hidden layer 7 had mean 0.00035636251219643234 and std 0.9817815950648252
hidden layer 8 had mean -0.0015383316857630897 and std 0.9817278825435273
hidden layer 9 had mean -0.00048497563819457115 and std 0.9817507524970054
hidden layer 10 had mean 0.0015641156086318114 and std 0.9818682873635451
```
<img src="./screenshot/08_nn05.png" width="500">

<img src="./screenshot/08_nn04.png" width="700">

Gradientê°€ 0ì´ ë˜ì–´í•™ìŠµì´ ì§„í–‰ë˜ì§€ ì•ŠëŠ”ë‹¤.   
ê°’ì´ ë„ˆë¬´ í´ ê²½ìš° ë¶„í¬ê°€ ì–‘ìª½ìœ¼ë¡œ íŠ„ë‹¤.

<br/>

ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™” í•˜ëŠ” ë°©ë²•ë“¤ì— ì•Œì•„ë³´ì.

### ì´ˆê¸°í™” 1 : Xavier Initialization

```python
    W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)
```

`Xavier` ì´ˆê¸°í™” ë°©ë²•ì€ í‘œì¤€ ì •ê·œ ë¶„í¬ë¥¼ ì…ë ¥ ê°œìˆ˜ì˜ í‘œì¤€ í¸ì°¨ë¡œ ë‚˜ëˆ„ëŠ” ë°©ë²•ì´ë‹¤.

```profile
input layer had mean -0.0007602850190879709 and std 0.9998665656626502
hidden layer 1 had mean 0.0002708052235341852 and std 0.6286386350311198
hidden layer 2 had mean 7.981811704091712e-05 and std 0.4871593737123403
hidden layer 3 had mean -5.8048525414097e-05 and std 0.4095861395151598
hidden layer 4 had mean -0.0003551174024725932 and std 0.35836212510095666
hidden layer 5 had mean -0.0005494540483613055 and std 0.32226268320263796
hidden layer 6 had mean -0.0009404907509430189 and std 0.2955054628386911
hidden layer 7 had mean -0.00031023383926448016 and std 0.27523767970783497
hidden layer 8 had mean -0.00033325822421282486 and std 0.25767757148442433
hidden layer 9 had mean -0.0006399647322567711 and std 0.24192558051206042
hidden layer 10 had mean 0.00011454739685321903 and std 0.22988941690330594
```
<img src="./screenshot/08_xavier1.png" width="500">

<img src="./screenshot/08_xavier2.png" width="700">

ì´ì „ ë…¸ë“œì™€ ë‹¤ìŒ ë…¸ë“œì˜ ê°œìˆ˜ì— ì˜ì¡´í•˜ëŠ” ë°©ë²•ì´ë‹¤. 

Gradient Vanishing í˜„ìƒì„ ì™„í™”í•˜ê¸° ìœ„í•´ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™” í•  ë•Œ `sigmoid` ì™€ ê°™ì€ s ì í•¨ìˆ˜ì˜ ê²½ìš° ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ì¶œë ¥ê°’ë“¤ì´ **í‘œì¤€ ì •ê·œ ë¶„í¬** í˜•íƒœë¥¼ ê°–ê²Œ í•˜ëŠ” ê²ƒì´ë‹¤. ì¶œë ¥ê°’ë“¤ì´ í‘œì¤€ ì •ê·œ ë¶„í¬ í˜•íƒœë¥¼ ê°–ê²Œ ë˜ì–´ì•¼ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

`Xavier` í•¨ìˆ˜ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ (`sigmoid`, `tanh`) ì—ì„œ íš¨ê³¼ì ì¸ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. 


```python
    nonlinearities = ['relu']*len(hidden_layer_sizes)
```

```python
    W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)
```
<img src="./screenshot/08_relu1.png" width="500">

<img src="./screenshot/08_relu2.png" width="700">

`Xavier` ì´ˆê¸°í™” ë°©ë²•ê³¼  `ReLU` í•¨ìˆ˜ë¥¼ ê²°í•©í–ˆì„ ë•Œ ê·¸ë˜í”„ì´ë‹¤. ì¶œë ¥ê°’, í‰ê· , í‘œì¤€í¸ì°¨ ëª¨ë‘ 0ìœ¼ë¡œ ìˆ˜ë ´í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.   
`ReLU` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ê²½ìš°ì—ëŠ” `Xavier` ì´ˆê¸°í™” ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.ã„´


### ì´ˆê¸°í™” 2 : He Initialization

```python
    W = np.random.randn(fan_in, fan_out) / np.sqrt(2/fan_in)
```
<img src="./screenshot/08_he1.png" width="500">

<img src="./screenshot/08_hes2.png" width="700">

`He` ì´ˆê¸°í™”ì™€ `ReLU` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œì˜ ê·¸ë˜í”„ì´ë‹¤. 10ì¸µ layerì—ì„œë„ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ê°€ ëª¨ë‘ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ì§€ ì•ŠëŠ”ë‹¤.


### Conclusion on Weight Initialization

- `sigmoid`, `tanh` : use `Xavier`
-`ReLU` : use `He`

<br/>

## Batch Normalization

**ë°°ì¹˜ ì •ê·œí™”** ëŠ” Activation Function ì˜ í™œì„±í™”ê°’ ë˜ëŠ” ì¶œë ¥ê°’ì„ ì •ê·œí™” (ì •ê·œ ë¶„í¬ë¡œ ë§Œë“¦) í•˜ëŠ” ì‘ì—…ì„ ë§í•œë‹¤. 

> ì‹ ê²½ë§ì„ í•™ìŠµì‹œí‚¬ ë•Œ ë³´í†µ ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— í•™ìŠµì‹œí‚¤ì§€ ì•Šê³  ì¡°ê·¸ë§Œ ë‹¨ìœ„ë¡œ ë¶„í• í•´ì„œ í•™ìŠµì„ ì‹œí‚¤ëŠ” ë° ì´ ë•Œ ì¡°ê·¸ë§Œ ë‹¨ìœ„ê°€ ë°°ì¹˜ë‹¤. 

ì‹ ê²½ë§ì˜ ê° layer ì—ì„œ ë°ì´í„° (ë°°ì¹˜) ì˜ ë¶„í¬ë¥¼ ì •ê·œí™” í•˜ëŠ” ì‘ì—…ì´ë‹¤.    

<img src="./screenshot/08_bn1.png" width="200">

<img src="./screenshot/08_bn2.png" width="500">

ê¹Šì€ ì‹ ê²½ë§ì¼ ìˆ˜ë¡ ê°™ì€ Input ê°’ì„ ê°–ë”ë¼ë„ ê°€ì¤‘ì¹˜ê°€ ì¡°ê¸ˆë§Œ ë‹¬ë¼ì§€ë©´ ì™„ì „íˆ ë‹¤ë¥¸ ê°’ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê° ì¸µì˜ ì¶œë ¥ê°’ì— ë°°ì¹˜ ì •ê·œí™” ê³¼ì •ì„ ì¶”ê°€í•´ì¤€ë‹¤ë©´ ê°€ì¤‘ì¹˜ì˜ ì°¨ì´ë¥¼ ì™„í™”í•˜ì—¬ ë³´ë‹¤ ì•ˆì •ì ì¸ í•™ìŠµì´ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤.   

ì…ë ¥ ë¶„í¬ê°€ ì¼ì •í•˜ê²Œ ë˜ì–´ Learning rate ë¥¼ í¬ê²Œ ì„¤ì •í•´ë„ ê´œì°®ì•„ì§„ë‹¤.

<img src="./screenshot/08_batch1.png" width="500">

ì¶œë ¥ê°’ì„ ì •ê·œí™” í•  ë•Œ í‰ê· , í‘œì¤€í¸ì°¨, ì–¼ë§ˆë‚˜ ì´ë™ì‹œí‚¬ì§€ ë“±ì˜ parameter ë“¤ ë˜í•œ ì—­ì „íŒŒë¥¼ í†µí•´ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.   
Gamma ì™€ Beta ë¥¼ í•™ìŠµí•˜ë„ë¡ í•˜ë©´ Normalize íš¨ê³¼ë¥¼ ì–´ëŠì •ë„ ì ìš©í•  ì§€ ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤.


### Conclusion on Batch Normalization

- í•™ìŠµ ì†ë„ê°€ ê°œì„ ëœë‹¤. (í•™ìŠµë¥ ì„ ë†’ê²Œ ì„¤ì •í•  ìˆ˜ ìˆê¸° ë•Œë¬¸)
- ê°€ì¤‘ì¹˜ ì´ˆê¹ƒê°’ ì„ íƒì˜ ì˜ì¡´ì„±ì´ ì ì–´ì§„ë‹¤. (í•™ìŠµì„ í•  ë•Œë§ˆë‹¤ ì¶œë ¥ê°’ì„ ì •ê·œí™”í•˜ê¸° ë•Œë¬¸)
- overfitting (ê³¼ì í•©) ìœ„í—˜ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤. drop out ê°™ì€ ê¸°ë²• ëŒ€ì²´ ê°€ëŠ¥
- Gradient Vanishing ë¬¸ì œ í•´ê²°ì´ ê°€ëŠ¥í•˜ë‹¤.

ê·¸ëŸ¬ë‹ˆê¹Œ ğŸ”¥ ì‚¬ìš©í•˜ëŠ”ê²Œ ì¢‹ë‹¤ ğŸ”¥

<br/>

## Summary âœï¸

- Activation Function : use ReLU
- Data preprocessing: subtract mean
- Weight Initialization: user Xavier / He init
- Batch Normalization : use
- Babysitting the learning process
- Hyperparameter optimization : random sample hyperparams, in log space when appropriate
